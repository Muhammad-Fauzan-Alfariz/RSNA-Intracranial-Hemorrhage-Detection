{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rsna_efficientnetb2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobassir94/RSNA-Intracranial-Hemorrhage-Detection/blob/master/rsna_efficientnetb2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FakRWDPtXfXs",
        "colab_type": "text"
      },
      "source": [
        "The DICOM format is so cool, but I prefer normal images :)\n",
        "\n",
        "With 156GB (compressed) it is very difficult to work with the resources of the vast majority of the mortals.\n",
        "This notebook shows you how to scale down all the images and create a new dataset easier to deal with.\n",
        "Even with the best computing resources, I don't think it's necessary to use the original size to get good accuracy.\n",
        "\n",
        "If you feel that you need bigger images or you want to store the images in another format you only need to change a couple of lines in the next section (Constants).\n",
        "\n",
        "Some code taken from:\n",
        "* https://www.kaggle.com/omission/eda-view-dicom-images-with-correct-windowing\n",
        "* https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/discussion/109649#latest-631701\n",
        "\n",
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvafClX-XrmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Desired output size.\n",
        "RESIZED_WIDTH, RESIZED_HEIGHT = 128, 128\n",
        "\n",
        "OUTPUT_FORMAT = \"png\"\n",
        "\n",
        "OUTPUT_DIR = \"output\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9VhPK8HoHt9",
        "colab_type": "text"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RQs59bLoL9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "# Uninstall and reinstall kaggle because I'm getting and error: the kaggle script\n",
        "# is running on Python 2 instead of Python 3 and fails when downloading kernel\n",
        "# outputs.\n",
        "# It's also needed for download data for this competition.\n",
        "!pip uninstall -y kaggle\n",
        "!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXxbGtzXB7BZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "# Install this library for reading the *.dcm images of this competition.\n",
        "!pip install pydicom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chgE9gVMQ4HP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "# Mount fuse-zip to mount zip files so we can access the files without unzip it.\n",
        "# This is needed because of the lack of space in Google Colab disk.\n",
        "!apt-get install -y fuse-zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inFQEaaMoKe2",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPNIQTvEmiXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "import joblib\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import PIL\n",
        "\n",
        "import pydicom\n",
        "\n",
        "import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAIvD7YYm7VB",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSQ1lTX4m8hS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set environment variables for using the Kaggle API.\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"my name\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"mykey\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQmKznCJn6lU",
        "colab_type": "text"
      },
      "source": [
        "# Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl-PZYtsnCOA",
        "colab_type": "code",
        "outputId": "4a5accd9-0733-473e-a10e-25ff62c3b6d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# 30-45min in Google Colab.\n",
        "raw_data_dir = \"input/raw\"\n",
        "!kaggle competitions download -c rsna-intracranial-hemorrhage-detection -p {raw_data_dir}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading rsna-intracranial-hemorrhage-detection.zip to input/raw\n",
            "100% 156G/156G [15:51<00:00, 180MB/s]\n",
            "100% 156G/156G [15:51<00:00, 176MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bQaxTBsUFeF",
        "colab_type": "text"
      },
      "source": [
        "# Mount ZIP with fuse-zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrHBOVyYUJZa",
        "colab_type": "code",
        "outputId": "249f928e-d778-4bd8-f39c-51ea26ef1a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "# Around 10 min in Google Colab.\n",
        "\n",
        "input_dir = \"/tmp/kaggle-data\"\n",
        "!mkdir {input_dir}\n",
        "!fuse-zip input/raw/rsna-intracranial-hemorrhage-detection.zip {input_dir}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.52 s, sys: 307 ms, total: 2.83 s\n",
            "Wall time: 9min 15s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE06xXqBWbws",
        "colab_type": "code",
        "outputId": "afae1de2-d63c-4493-ec07-c1c2aee48c3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Check that everything is working.\n",
        "!ls {input_dir}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stage_1_sample_submission.csv  stage_1_train.csv\n",
            "stage_1_test_images\t       stage_1_train_images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqo9cUAiU6Eo",
        "colab_type": "text"
      },
      "source": [
        "# Get images path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FsLeV_iYjdM",
        "colab_type": "code",
        "outputId": "17a2a129-91ba-4c48-a772-f46d336b743c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_dir = \"stage_1_train_images/\"\n",
        "train_paths = glob.glob(f\"{input_dir}/{train_dir}/*.dcm\")\n",
        "test_dir = \"stage_1_test_images/\"\n",
        "test_paths = glob.glob(f\"{input_dir}/{test_dir}/*.dcm\")\n",
        "len(train_paths), len(test_paths)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(674258, 78545)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_Vmp18B7YuO",
        "colab_type": "code",
        "outputId": "cab124a0-c9f9-45f2-97ea-7b8e5a28c807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "import os\n",
        "import collections\n",
        "import sys\n",
        "import glob\n",
        "import random\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import multiprocessing\n",
        "\n",
        "from math import ceil, floor\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "from keras.models import Model, load_model\n",
        "from keras.utils import Sequence\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from google.colab import files\n",
        "\n",
        "# Install Modules from internet\n",
        "!pip install efficientnet\n",
        "!pip install iterative-stratification\n",
        "\n",
        "# Import Custom Modules\n",
        "import efficientnet.keras as efn \n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet\n",
            "  Downloading https://files.pythonhosted.org/packages/97/82/f3ae07316f0461417dc54affab6e86ab188a5a22f33176d35271628b96e0/efficientnet-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from efficientnet) (1.0.8)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from efficientnet) (0.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.17.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (2.8.0)\n",
            "Requirement already satisfied: imageio>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (2.4.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (4.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (2.4)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (3.1.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (1.1.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.12.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.3.0->scikit-image->efficientnet) (0.46)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->efficientnet) (4.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (41.4.0)\n",
            "Installing collected packages: efficientnet\n",
            "Successfully installed efficientnet-1.0.0\n",
            "Collecting iterative-stratification\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.17.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.14.0)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qadY9KA27Yqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seed\n",
        "SEED = 12345\n",
        "np.random.seed(SEED)\n",
        "#tf.set_random_seed(SEED)\n",
        "\n",
        "# Constants\n",
        "TEST_SIZE = 0.15\n",
        "HEIGHT = 256\n",
        "WIDTH = 256\n",
        "CHANNELS = 3\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 64\n",
        "SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
        "\n",
        "# Folders\n",
        "#DATA_DIR = '/kaggle/input/rsna-intracranial-hemorrhage-detection/'\n",
        "TEST_IMAGES_DIR = test_dir\n",
        "TRAIN_IMAGES_DIR = train_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMnLIunV7Yox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correct_dcm(dcm):\n",
        "    x = dcm.pixel_array + 1000\n",
        "    px_mode = 4096\n",
        "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
        "    dcm.PixelData = x.tobytes()\n",
        "    dcm.RescaleIntercept = -1000\n",
        "\n",
        "def window_image(dcm, window_center, window_width):    \n",
        "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
        "        correct_dcm(dcm)\n",
        "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
        "    \n",
        "    # Resize\n",
        "    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n",
        "   \n",
        "    img_min = window_center - window_width // 2\n",
        "    img_max = window_center + window_width // 2\n",
        "    img = np.clip(img, img_min, img_max)\n",
        "    return img\n",
        "\n",
        "def bsb_window(dcm):\n",
        "    brain_img = window_image(dcm, 40, 80)\n",
        "    subdural_img = window_image(dcm, 80, 200)\n",
        "    soft_img = window_image(dcm, 40, 380)\n",
        "    \n",
        "    brain_img = (brain_img - 0) / 80\n",
        "    subdural_img = (subdural_img - (-20)) / 200\n",
        "    soft_img = (soft_img - (-150)) / 380\n",
        "    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n",
        "    return bsb_img\n",
        "\n",
        "def _read(path, SHAPE):\n",
        "    dcm = pydicom.dcmread(path)\n",
        "    try:\n",
        "        img = bsb_window(dcm)\n",
        "    except:\n",
        "        img = np.zeros(SHAPE)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55q8GQZDGawc",
        "colab_type": "code",
        "outputId": "af593885-c6b0-40ee-b210-e2daf25905e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(os.listdir('./input/raw'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['rsna-intracranial-hemorrhage-detection.zip']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1WpH-Z27YlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = \"/content/drive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsyuE1lZ7Ygs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Image Augmentation\n",
        "sometimes = lambda aug: iaa.Sometimes(0.25, aug)\n",
        "augmentation = iaa.Sequential([ iaa.Fliplr(0.25),\n",
        "                                iaa.Flipud(0.10),\n",
        "                                sometimes(iaa.Crop(px=(0, 25), keep_size = True, sample_independently = False))   \n",
        "                            ], random_order = True)       \n",
        "        \n",
        "# Generators\n",
        "class TrainDataGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, dataset, labels, batch_size = 16, img_size = SHAPE, img_dir = TRAIN_IMAGES_DIR, augment = False, *args, **kwargs):\n",
        "        self.dataset = dataset\n",
        "        self.ids = dataset.index\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.img_dir = input_dir+\"/\"+img_dir\n",
        "        self.augment = augment\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(ceil(len(self.ids) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        X, Y = self.__data_generation(indices)\n",
        "        return X, Y\n",
        "\n",
        "    def augmentor(self, image):\n",
        "        augment_img = augmentation        \n",
        "        image_aug = augment_img.augment_image(image)\n",
        "        return image_aug\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indices = np.arange(len(self.ids))\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "    def __data_generation(self, indices):\n",
        "        X = np.empty((self.batch_size, *self.img_size))\n",
        "        Y = np.empty((self.batch_size, 6), dtype=np.float32)\n",
        "        \n",
        "        for i, index in enumerate(indices):\n",
        "            ID = self.ids[index]\n",
        "            image = _read(self.img_dir+ID+\".dcm\", self.img_size)########\n",
        "            if self.augment:\n",
        "                X[i,] = self.augmentor(image)\n",
        "            else:\n",
        "                X[i,] = image\n",
        "            Y[i,] = self.labels.iloc[index].values        \n",
        "        return X, Y\n",
        "    \n",
        "class TestDataGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, dataset, labels, batch_size = 16, img_size = SHAPE, img_dir = TEST_IMAGES_DIR, *args, **kwargs):\n",
        "        self.dataset = dataset\n",
        "        self.ids = dataset.index\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.img_dir = input_dir+\"/\"+img_dir\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(ceil(len(self.ids) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        X = self.__data_generation(indices)\n",
        "        return X\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indices = np.arange(len(self.ids))\n",
        "    \n",
        "    def __data_generation(self, indices):\n",
        "        X = np.empty((self.batch_size, *self.img_size))\n",
        "        \n",
        "        for i, index in enumerate(indices):\n",
        "            ID = self.ids[index]\n",
        "            image = _read(self.img_dir+ID+\".dcm\", self.img_size)########\n",
        "            X[i,] = image              \n",
        "        return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fddS33gm7Yed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_testset(filename = input_dir + \"/stage_1_sample_submission.csv\"):\n",
        "    df = pd.read_csv(filename)\n",
        "    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n",
        "    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n",
        "    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n",
        "    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n",
        "    return df\n",
        "\n",
        "def read_trainset(filename = input_dir + \"/stage_1_train.csv\"):\n",
        "    df = pd.read_csv(filename)\n",
        "    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n",
        "    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n",
        "    duplicates_to_remove = [\n",
        "        1598538, 1598539, 1598540, 1598541, 1598542, 1598543,\n",
        "        312468,  312469,  312470,  312471,  312472,  312473,\n",
        "        2708700, 2708701, 2708702, 2708703, 2708704, 2708705,\n",
        "        3032994, 3032995, 3032996, 3032997, 3032998, 3032999\n",
        "    ]\n",
        "    df = df.drop(index = duplicates_to_remove)\n",
        "    df = df.reset_index(drop = True)    \n",
        "    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n",
        "    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n",
        "    return df\n",
        "\n",
        "# Read Train and Test Datasets\n",
        "test_df = read_testset()\n",
        "train_df = read_trainset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t6J8SCR7YbM",
        "colab_type": "code",
        "outputId": "db35bf80-738f-4ef2-9600-31082671385f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Oversampling\n",
        "epidural_df = train_df[train_df.Label['epidural'] == 1]\n",
        "train_oversample_df = pd.concat([train_df, epidural_df])\n",
        "train_df = train_oversample_df\n",
        "\n",
        "# Summary\n",
        "print('Train Shape: {}'.format(train_df.shape))\n",
        "print('Test Shape: {}'.format(test_df.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Shape: (677019, 6)\n",
            "Test Shape: (78545, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzHwrTVxBrrL",
        "colab_type": "code",
        "outputId": "31526b10-d77c-414b-f7ba-6ae410e279aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crF_cyPD7YZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predictions(test_df, model):    \n",
        "    test_preds = model.predict_generator(TestDataGenerator(test_df, None, 5, SHAPE, TEST_IMAGES_DIR), verbose = 1)\n",
        "    return test_preds[:test_df.iloc[range(test_df.shape[0])].shape[0]]\n",
        "\n",
        "def ModelCheckpointFull(model_name):\n",
        "    return ModelCheckpoint(filepath + model_name, \n",
        "                            monitor = 'val_loss', \n",
        "                            verbose = 1, \n",
        "                            save_best_only = False, \n",
        "                            save_weights_only = True, \n",
        "                            mode = 'min', \n",
        "                            period = 1)\n",
        "\n",
        "# Create Model\n",
        "def create_model():\n",
        "    #K.clear_session()\n",
        "    \n",
        "    base_model =  efn.EfficientNetB2(weights = 'imagenet', include_top = False, pooling = 'avg', input_shape = SHAPE)\n",
        "    #base_model.load_weights(filepath+'model.h5')\n",
        "    x = base_model.output\n",
        "    x = Dropout(0.10)(x)\n",
        "    y_pred = Dense(6, activation = 'sigmoid')(x)\n",
        "\n",
        "    return Model(inputs = base_model.input, outputs = y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzGG1OYD7YV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Submission Placeholder\n",
        "submission_predictions = []\n",
        "\n",
        "# Multi Label Stratified Split stuff...\n",
        "msss = MultilabelStratifiedShuffleSplit(n_splits = 10, test_size = TEST_SIZE, random_state = SEED)\n",
        "X = train_df.index\n",
        "Y = train_df.Label.values\n",
        "\n",
        "# Get train and test index\n",
        "msss_splits = next(msss.split(X, Y))\n",
        "train_idx = msss_splits[0]\n",
        "valid_idx = msss_splits[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWzXsFfa7YTa",
        "colab_type": "code",
        "outputId": "51d60ec4-649b-490e-a8fa-fa3e6a16dd2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        }
      },
      "source": [
        "# Loop through Folds of Multi Label Stratified Split\n",
        "#for epoch, msss_splits in zip(range(0, 9), msss.split(X, Y)): \n",
        "#    # Get train and test index\n",
        "#    train_idx = msss_splits[0]\n",
        "#    valid_idx = msss_splits[1]\n",
        "LR = 0.00051\n",
        "for epoch in range(0, 7):\n",
        "    print('=========== EPOCH {}'.format(epoch))\n",
        "\n",
        "    # Shuffle Train data\n",
        "    np.random.shuffle(train_idx)\n",
        "    print(train_idx[:5])    \n",
        "    print(valid_idx[:5])\n",
        "\n",
        "    # Create Data Generators for Train and Valid\n",
        "    data_generator_train = TrainDataGenerator(train_df.iloc[train_idx], \n",
        "                                                train_df.iloc[train_idx], \n",
        "                                                TRAIN_BATCH_SIZE, \n",
        "                                                SHAPE,\n",
        "                                                augment = True)\n",
        "    data_generator_val = TrainDataGenerator(train_df.iloc[valid_idx], \n",
        "                                            train_df.iloc[valid_idx], \n",
        "                                            VALID_BATCH_SIZE, \n",
        "                                            SHAPE,\n",
        "                                            augment = False)\n",
        "\n",
        "    # Create Model\n",
        "    model = create_model()\n",
        "    \n",
        "    # Full Training Model\n",
        "    for base_layer in model.layers[:-1]:\n",
        "      base_layer.trainable = True\n",
        "      TRAIN_STEPS = int(len(data_generator_train) / 6)\n",
        "      model.load_weights(filepath + \"model.h5\")\n",
        "      # Load Model Weights\n",
        "      '''if epoch != 0:\n",
        "        a = filepath + \"model.h5\"\n",
        "        model.load_weights(a)'''  \n",
        "        \n",
        "\n",
        "    model.compile(optimizer = Adam(LR), \n",
        "                  loss = 'binary_crossentropy',\n",
        "                  metrics = ['acc'])\n",
        "    \n",
        "    # Train Model\n",
        "    model.fit_generator(generator = data_generator_train,\n",
        "                        validation_data = data_generator_val,\n",
        "                        steps_per_epoch = TRAIN_STEPS,\n",
        "                        epochs = 1,\n",
        "                        callbacks = [ModelCheckpointFull('model.h5')],\n",
        "                        verbose = 1)\n",
        "    \n",
        "    # Starting with the 6th epoch we create predictions for the test set on each epoch\n",
        "    if epoch >= 1:\n",
        "        preds = predictions(test_df, model)\n",
        "        submission_predictions.append(preds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========== EPOCH 0\n",
            "[125803 546040 567989 379531 599105]\n",
            "[ 0 17 27 30 44]\n",
            "Epoch 1/1\n",
            "2997/2997 [==============================] - 5209s 2s/step - loss: 0.0763 - acc: 0.9729 - val_loss: 0.0758 - val_acc: 0.9735\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/model.h5\n",
            "=========== EPOCH 1\n",
            "[672115 591959 279174 579813 223156]\n",
            "[ 0 17 27 30 44]\n",
            "Epoch 1/1\n",
            "2997/2997 [==============================] - 5309s 2s/step - loss: 0.0741 - acc: 0.9737 - val_loss: 0.0720 - val_acc: 0.9737\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/model.h5\n",
            "15709/15709 [==============================] - 1576s 100ms/step\n",
            "=========== EPOCH 2\n",
            "[664782 378233 424470 674642  50041]\n",
            "[ 0 17 27 30 44]\n",
            "Epoch 1/1\n",
            "2997/2997 [==============================] - 5323s 2s/step - loss: 0.0724 - acc: 0.9743 - val_loss: 0.0689 - val_acc: 0.9751\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/model.h5\n",
            "15709/15709 [==============================] - 1582s 101ms/step\n",
            "=========== EPOCH 3\n",
            "[470367 389402  56407 448635 350856]\n",
            "[ 0 17 27 30 44]\n",
            "Epoch 1/1\n",
            "2997/2997 [==============================] - 5312s 2s/step - loss: 0.0718 - acc: 0.9745 - val_loss: 0.0678 - val_acc: 0.9756\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/model.h5\n",
            "15709/15709 [==============================] - 1571s 100ms/step\n",
            "=========== EPOCH 4\n",
            "[202010 460334 372411 591705 233296]\n",
            "[ 0 17 27 30 44]\n",
            "Epoch 1/1\n",
            "2814/2997 [===========================>..] - ETA: 3:27 - loss: 0.0700 - acc: 0.9746Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjBH-1Ia7YQW",
        "colab_type": "code",
        "outputId": "53f358fd-1f41-41d1-ece4-b3c7c3618783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "test_df.iloc[:, :] = np.average(submission_predictions, axis = 0, weights = [2**i for i in range(len(submission_predictions))])\n",
        "test_df = test_df.stack().reset_index()\n",
        "test_df.insert(loc = 0, column = 'ID', value = test_df['Image'].astype(str) + \"_\" + test_df['Diagnosis'])\n",
        "test_df = test_df.drop([\"Image\", \"Diagnosis\"], axis=1)\n",
        "test_df.to_csv('submission.csv', index = False)\n",
        "print(test_df.head(12))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               ID     Label\n",
            "0                ID_000012eaf_any  0.047340\n",
            "1           ID_000012eaf_epidural  0.001187\n",
            "2   ID_000012eaf_intraparenchymal  0.010457\n",
            "3   ID_000012eaf_intraventricular  0.002544\n",
            "4       ID_000012eaf_subarachnoid  0.008325\n",
            "5           ID_000012eaf_subdural  0.030030\n",
            "6                ID_0000ca2f6_any  0.006918\n",
            "7           ID_0000ca2f6_epidural  0.000246\n",
            "8   ID_0000ca2f6_intraparenchymal  0.001239\n",
            "9   ID_0000ca2f6_intraventricular  0.001386\n",
            "10      ID_0000ca2f6_subarachnoid  0.001098\n",
            "11          ID_0000ca2f6_subdural  0.002256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCnE-sUd7YOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!cp submission.csv drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1ZFIUkF7YMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}