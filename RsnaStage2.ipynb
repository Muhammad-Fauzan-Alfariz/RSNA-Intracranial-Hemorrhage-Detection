{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resize dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FakRWDPtXfXs",
        "colab_type": "text"
      },
      "source": [
        "The DICOM format is so cool, but I prefer normal images :)\n",
        "\n",
        "With 156GB (compressed) it is very difficult to work with the resources of the vast majority of the mortals.\n",
        "This notebook shows you how to scale down all the images and create a new dataset easier to deal with.\n",
        "Even with the best computing resources, I don't think it's necessary to use the original size to get good accuracy.\n",
        "\n",
        "If you feel that you need bigger images or you want to store the images in another format you only need to change a couple of lines in the next section (Constants).\n",
        "\n",
        "Some code taken from:\n",
        "* https://www.kaggle.com/omission/eda-view-dicom-images-with-correct-windowing\n",
        "* https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/discussion/109649#latest-631701\n",
        "\n",
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvafClX-XrmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Desired output size.\n",
        "RESIZED_WIDTH, RESIZED_HEIGHT = 128, 128\n",
        "\n",
        "OUTPUT_FORMAT = \"png\"\n",
        "\n",
        "OUTPUT_DIR = \"output\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9VhPK8HoHt9",
        "colab_type": "text"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RQs59bLoL9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "# Uninstall and reinstall kaggle because I'm getting and error: the kaggle script\n",
        "# is running on Python 2 instead of Python 3 and fails when downloading kernel\n",
        "# outputs.\n",
        "# It's also needed for download data for this competition.\n",
        "!pip uninstall -y kaggle\n",
        "!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXxbGtzXB7BZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "# Install this library for reading the *.dcm images of this competition.\n",
        "!pip install pydicom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chgE9gVMQ4HP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "# Mount fuse-zip to mount zip files so we can access the files without unzip it.\n",
        "# This is needed because of the lack of space in Google Colab disk.\n",
        "!apt-get install -y fuse-zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inFQEaaMoKe2",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPNIQTvEmiXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "import joblib\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import PIL\n",
        "\n",
        "import pydicom\n",
        "\n",
        "import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAIvD7YYm7VB",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSQ1lTX4m8hS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set environment variables for using the Kaggle API.\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"mobassir\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"hidden\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQmKznCJn6lU",
        "colab_type": "text"
      },
      "source": [
        "# Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl-PZYtsnCOA",
        "colab_type": "code",
        "outputId": "433e1ca3-cd1c-413e-8406-df5830507354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# 30-45min in Google Colab.\n",
        "raw_data_dir = \"input/raw\"\n",
        "!kaggle competitions download -c rsna-intracranial-hemorrhage-detection -p {raw_data_dir}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading rsna-intracranial-hemorrhage-detection.zip to input/raw\n",
            "100% 181G/181G [1:25:41<00:00, 26.3MB/s]\n",
            "100% 181G/181G [1:25:41<00:00, 37.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bQaxTBsUFeF",
        "colab_type": "text"
      },
      "source": [
        "# Mount ZIP with fuse-zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrHBOVyYUJZa",
        "colab_type": "code",
        "outputId": "0c461983-98cd-4599-c086-c6a064968e40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "# Around 10 min in Google Colab.\n",
        "\n",
        "input_dir = \"/tmp/kaggle-data\"\n",
        "!mkdir {input_dir}\n",
        "!fuse-zip input/raw/rsna-intracranial-hemorrhage-detection.zip {input_dir}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.58 s, sys: 363 ms, total: 2.94 s\n",
            "Wall time: 8min 30s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE06xXqBWbws",
        "colab_type": "code",
        "outputId": "9e2490d1-a6dd-4555-ec4d-724df3d5b775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Check that everything is working.\n",
        "!ls {input_dir}"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stage_2_sample_submission.csv  stage_2_train.csv\n",
            "stage_2_test_images\t       stage_2_train_images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqo9cUAiU6Eo",
        "colab_type": "text"
      },
      "source": [
        "# Get images path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FsLeV_iYjdM",
        "colab_type": "code",
        "outputId": "740a68f4-34ec-4543-820b-956a130f71d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_dir = \"stage_2_train_images/\"\n",
        "train_paths = glob.glob(f\"{input_dir}/{train_dir}/*.dcm\")\n",
        "test_dir = \"stage_2_test_images/\"\n",
        "test_paths = glob.glob(f\"{input_dir}/{test_dir}/*.dcm\")\n",
        "len(train_paths), len(test_paths)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(752803, 121232)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_Vmp18B7YuO",
        "colab_type": "code",
        "outputId": "209dd87c-dede-4428-e202-c8c7dd55de6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "import os\n",
        "import collections\n",
        "import sys\n",
        "import glob\n",
        "import random\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import multiprocessing\n",
        "\n",
        "from math import ceil, floor\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "from keras.models import Model, load_model\n",
        "from keras.utils import Sequence\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from google.colab import files\n",
        "\n",
        "# Install Modules from internet\n",
        "!pip install efficientnet\n",
        "!pip install iterative-stratification\n",
        "\n",
        "# Import Custom Modules\n",
        "import efficientnet.keras as efn \n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet\n",
            "  Downloading https://files.pythonhosted.org/packages/97/82/f3ae07316f0461417dc54affab6e86ab188a5a22f33176d35271628b96e0/efficientnet-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from efficientnet) (0.15.0)\n",
            "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from efficientnet) (1.0.8)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (4.3.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (1.3.1)\n",
            "Requirement already satisfied: imageio>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (2.4)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (1.1.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.17.3)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.3.0->scikit-image->efficientnet) (0.46)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->efficientnet) (4.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (41.4.0)\n",
            "Installing collected packages: efficientnet\n",
            "Successfully installed efficientnet-1.0.0\n",
            "Collecting iterative-stratification\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.17.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.14.0)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qadY9KA27Yqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seed\n",
        "SEED = 12345\n",
        "np.random.seed(SEED)\n",
        "#tf.set_random_seed(SEED)\n",
        "\n",
        "# Constants\n",
        "TEST_SIZE = 0.15\n",
        "HEIGHT = 256\n",
        "WIDTH = 256\n",
        "CHANNELS = 3\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 64\n",
        "SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
        "\n",
        "# Folders\n",
        "#DATA_DIR = '/kaggle/input/rsna-intracranial-hemorrhage-detection/'\n",
        "TEST_IMAGES_DIR = test_dir\n",
        "TRAIN_IMAGES_DIR = train_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMnLIunV7Yox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correct_dcm(dcm):\n",
        "    x = dcm.pixel_array + 1000\n",
        "    px_mode = 4096\n",
        "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
        "    dcm.PixelData = x.tobytes()\n",
        "    dcm.RescaleIntercept = -1000\n",
        "\n",
        "def window_image(dcm, window_center, window_width):    \n",
        "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
        "        correct_dcm(dcm)\n",
        "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
        "    \n",
        "    # Resize\n",
        "    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n",
        "   \n",
        "    img_min = window_center - window_width // 2\n",
        "    img_max = window_center + window_width // 2\n",
        "    img = np.clip(img, img_min, img_max)\n",
        "    return img\n",
        "\n",
        "def bsb_window(dcm):\n",
        "    brain_img = window_image(dcm, 40, 80)\n",
        "    subdural_img = window_image(dcm, 80, 200)\n",
        "    soft_img = window_image(dcm, 40, 380)\n",
        "    \n",
        "    brain_img = (brain_img - 0) / 80\n",
        "    subdural_img = (subdural_img - (-20)) / 200\n",
        "    soft_img = (soft_img - (-150)) / 380\n",
        "    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n",
        "    return bsb_img\n",
        "\n",
        "def _read(path, SHAPE):\n",
        "    dcm = pydicom.dcmread(path)\n",
        "    try:\n",
        "        img = bsb_window(dcm)\n",
        "    except:\n",
        "        img = np.zeros(SHAPE)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55q8GQZDGawc",
        "colab_type": "code",
        "outputId": "371436c6-9054-4694-e0c0-1ce8a1fd74f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(os.listdir('./input/raw'))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['rsna-intracranial-hemorrhage-detection.zip']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1WpH-Z27YlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = \"/content/drive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsyuE1lZ7Ygs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Image Augmentation\n",
        "sometimes = lambda aug: iaa.Sometimes(0.25, aug)\n",
        "augmentation = iaa.Sequential([ iaa.Fliplr(0.25),\n",
        "                                iaa.Flipud(0.10),\n",
        "                                sometimes(iaa.Crop(px=(0, 25), keep_size = True, sample_independently = False))   \n",
        "                            ], random_order = True)       \n",
        "        \n",
        "# Generators\n",
        "class TrainDataGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, dataset, labels, batch_size = 16, img_size = SHAPE, img_dir = TRAIN_IMAGES_DIR, augment = False, *args, **kwargs):\n",
        "        self.dataset = dataset\n",
        "        self.ids = dataset.index\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.img_dir = input_dir+\"/\"+img_dir\n",
        "        self.augment = augment\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(ceil(len(self.ids) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        X, Y = self.__data_generation(indices)\n",
        "        return X, Y\n",
        "\n",
        "    def augmentor(self, image):\n",
        "        augment_img = augmentation        \n",
        "        image_aug = augment_img.augment_image(image)\n",
        "        return image_aug\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indices = np.arange(len(self.ids))\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "    def __data_generation(self, indices):\n",
        "        X = np.empty((self.batch_size, *self.img_size))\n",
        "        Y = np.empty((self.batch_size, 6), dtype=np.float32)\n",
        "        \n",
        "        for i, index in enumerate(indices):\n",
        "            ID = self.ids[index]\n",
        "            image = _read(self.img_dir+ID+\".dcm\", self.img_size)########\n",
        "            if self.augment:\n",
        "                X[i,] = self.augmentor(image)\n",
        "            else:\n",
        "                X[i,] = image\n",
        "            Y[i,] = self.labels.iloc[index].values        \n",
        "        return X, Y\n",
        "    \n",
        "class TestDataGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, dataset, labels, batch_size = 16, img_size = SHAPE, img_dir = TEST_IMAGES_DIR, *args, **kwargs):\n",
        "        self.dataset = dataset\n",
        "        self.ids = dataset.index\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.img_dir = input_dir+\"/\"+img_dir\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(ceil(len(self.ids) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        X = self.__data_generation(indices)\n",
        "        return X\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indices = np.arange(len(self.ids))\n",
        "    \n",
        "    def __data_generation(self, indices):\n",
        "        X = np.empty((self.batch_size, *self.img_size))\n",
        "        \n",
        "        for i, index in enumerate(indices):\n",
        "            ID = self.ids[index]\n",
        "            image = _read(self.img_dir+ID+\".dcm\", self.img_size)########\n",
        "            X[i,] = image              \n",
        "        return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S76bWDBKNIfd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "42990881-6b84-45fe-fcbb-2a7b77695d19"
      },
      "source": [
        "a = pd.read_csv(input_dir + \"/stage_2_train.csv\")\n",
        "a.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ID_12cadc6af_epidural</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ID_12cadc6af_intraparenchymal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ID_12cadc6af_intraventricular</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ID_12cadc6af_subarachnoid</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ID_12cadc6af_subdural</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              ID  Label\n",
              "0          ID_12cadc6af_epidural      0\n",
              "1  ID_12cadc6af_intraparenchymal      0\n",
              "2  ID_12cadc6af_intraventricular      0\n",
              "3      ID_12cadc6af_subarachnoid      0\n",
              "4          ID_12cadc6af_subdural      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ckrlFOQWD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_bigdata_duplicates = a[a.duplicated()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVGQ2i1dQjYB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b88f552-1763-4b4f-97df-0f7f371a39b9"
      },
      "source": [
        "a.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4516842, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN8xXshxQnxt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "52482398-66f2-4fd9-e6c6-34e82ee94161"
      },
      "source": [
        "df_bigdata_duplicates"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>56346</th>\n",
              "      <td>ID_a64d5deed_epidural</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56347</th>\n",
              "      <td>ID_a64d5deed_intraparenchymal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56348</th>\n",
              "      <td>ID_a64d5deed_intraventricular</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56349</th>\n",
              "      <td>ID_a64d5deed_subarachnoid</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56350</th>\n",
              "      <td>ID_a64d5deed_subdural</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56351</th>\n",
              "      <td>ID_a64d5deed_any</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1171830</th>\n",
              "      <td>ID_854fba667_epidural</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1171831</th>\n",
              "      <td>ID_854fba667_intraparenchymal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1171832</th>\n",
              "      <td>ID_854fba667_intraventricular</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1171833</th>\n",
              "      <td>ID_854fba667_subarachnoid</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1171834</th>\n",
              "      <td>ID_854fba667_subdural</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1171835</th>\n",
              "      <td>ID_854fba667_any</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3705312</th>\n",
              "      <td>ID_489ae4179_epidural</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3705313</th>\n",
              "      <td>ID_489ae4179_intraparenchymal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3705314</th>\n",
              "      <td>ID_489ae4179_intraventricular</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3705315</th>\n",
              "      <td>ID_489ae4179_subarachnoid</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3705316</th>\n",
              "      <td>ID_489ae4179_subdural</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3705317</th>\n",
              "      <td>ID_489ae4179_any</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3842478</th>\n",
              "      <td>ID_921490062_epidural</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3842479</th>\n",
              "      <td>ID_921490062_intraparenchymal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3842480</th>\n",
              "      <td>ID_921490062_intraventricular</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3842481</th>\n",
              "      <td>ID_921490062_subarachnoid</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3842482</th>\n",
              "      <td>ID_921490062_subdural</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3842483</th>\n",
              "      <td>ID_921490062_any</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    ID  Label\n",
              "56346            ID_a64d5deed_epidural      0\n",
              "56347    ID_a64d5deed_intraparenchymal      0\n",
              "56348    ID_a64d5deed_intraventricular      0\n",
              "56349        ID_a64d5deed_subarachnoid      0\n",
              "56350            ID_a64d5deed_subdural      0\n",
              "56351                 ID_a64d5deed_any      0\n",
              "1171830          ID_854fba667_epidural      0\n",
              "1171831  ID_854fba667_intraparenchymal      0\n",
              "1171832  ID_854fba667_intraventricular      0\n",
              "1171833      ID_854fba667_subarachnoid      0\n",
              "1171834          ID_854fba667_subdural      0\n",
              "1171835               ID_854fba667_any      0\n",
              "3705312          ID_489ae4179_epidural      0\n",
              "3705313  ID_489ae4179_intraparenchymal      0\n",
              "3705314  ID_489ae4179_intraventricular      0\n",
              "3705315      ID_489ae4179_subarachnoid      0\n",
              "3705316          ID_489ae4179_subdural      0\n",
              "3705317               ID_489ae4179_any      0\n",
              "3842478          ID_921490062_epidural      0\n",
              "3842479  ID_921490062_intraparenchymal      0\n",
              "3842480  ID_921490062_intraventricular      0\n",
              "3842481      ID_921490062_subarachnoid      0\n",
              "3842482          ID_921490062_subdural      0\n",
              "3842483               ID_921490062_any      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fddS33gm7Yed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_testset(filename = input_dir + \"/stage_2_sample_submission.csv\"):\n",
        "    df = pd.read_csv(filename)\n",
        "    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n",
        "    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n",
        "    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n",
        "    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n",
        "    return df\n",
        "\n",
        "def read_trainset(filename = input_dir + \"/stage_2_train.csv\"):\n",
        "    df = pd.read_csv(filename)\n",
        "    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n",
        "    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n",
        "    #duplicates_to_remove = [\n",
        "     #   1598538, 1598539, 1598540, 1598541, 1598542, 1598543,\n",
        "     #   312468,  312469,  312470,  312471,  312472,  312473,\n",
        "     #   2708700, 2708701, 2708702, 2708703, 2708704, 2708705,\n",
        "     #   3032994, 3032995, 3032996, 3032997, 3032998, 3032999\n",
        "  #  ]\n",
        "    #df = df.drop(index = duplicates_to_remove)\n",
        "\n",
        "    duplicates_to_remove = [\n",
        "        56346,56347,56348,56349,\n",
        "        56350,56351,1171830,1171831,\n",
        "        1171832,1171833,1171834,1171835,\n",
        "        3705312,3705313,3705314,3705315,\n",
        "        3705316,3705317,3842478,3842479,\n",
        "        3842480,3842481,3842482,3842483\n",
        "    ]\n",
        "    df = df.drop(index = duplicates_to_remove)\n",
        "\n",
        "    df = df.reset_index(drop = True)    \n",
        "    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n",
        "    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n",
        "    return df\n",
        "\n",
        "# Read Train and Test Datasets\n",
        "test_df = read_testset()\n",
        "train_df = read_trainset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t6J8SCR7YbM",
        "colab_type": "code",
        "outputId": "74a53aa5-bc2a-4839-f7f8-ef3e9002c63e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Oversampling\n",
        "epidural_df = train_df[train_df.Label['epidural'] == 1]\n",
        "train_oversample_df = pd.concat([train_df, epidural_df])\n",
        "train_df = train_oversample_df\n",
        "\n",
        "# Summary\n",
        "print('Train Shape: {}'.format(train_df.shape))\n",
        "print('Test Shape: {}'.format(test_df.shape))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Shape: (755948, 6)\n",
            "Test Shape: (121232, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzHwrTVxBrrL",
        "colab_type": "code",
        "outputId": "e0df3464-0810-4215-9dbe-5f90635024e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crF_cyPD7YZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predictions(test_df, model):    \n",
        "    test_preds = model.predict_generator(TestDataGenerator(test_df, None, 5, SHAPE, TEST_IMAGES_DIR), verbose = 1)\n",
        "    return test_preds[:test_df.iloc[range(test_df.shape[0])].shape[0]]\n",
        "\n",
        "def ModelCheckpointFull(model_name):\n",
        "    return ModelCheckpoint(filepath + model_name, \n",
        "                            monitor = 'val_loss', \n",
        "                            verbose = 1, \n",
        "                            save_best_only = False, \n",
        "                            save_weights_only = True, \n",
        "                            mode = 'min', \n",
        "                            period = 1)\n",
        "\n",
        "# Create Model\n",
        "def create_model():\n",
        "    #K.clear_session()\n",
        "    \n",
        "    base_model =  efn.EfficientNetB2(weights = 'imagenet', include_top = False, pooling = 'avg', input_shape = SHAPE)\n",
        "    #base_model.load_weights(filepath+'model.h5')\n",
        "    x = base_model.output\n",
        "    x = Dropout(0.10)(x)\n",
        "    y_pred = Dense(6, activation = 'sigmoid')(x)\n",
        "\n",
        "    return Model(inputs = base_model.input, outputs = y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzGG1OYD7YV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Submission Placeholder\n",
        "submission_predictions = []\n",
        "\n",
        "# Multi Label Stratified Split stuff...\n",
        "msss = MultilabelStratifiedShuffleSplit(n_splits = 10, test_size = TEST_SIZE, random_state = SEED)\n",
        "X = train_df.index\n",
        "Y = train_df.Label.values\n",
        "\n",
        "# Get train and test index\n",
        "msss_splits = next(msss.split(X, Y))\n",
        "train_idx = msss_splits[0]\n",
        "valid_idx = msss_splits[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWzXsFfa7YTa",
        "colab_type": "code",
        "outputId": "581d5ef9-47a7-4794-c9ff-667aa4afda61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Loop through Folds of Multi Label Stratified Split\n",
        "#for epoch, msss_splits in zip(range(0, 9), msss.split(X, Y)): \n",
        "#    # Get train and test index\n",
        "#    train_idx = msss_splits[0]\n",
        "#    valid_idx = msss_splits[1]\n",
        "LR = 0.00051\n",
        "for epoch in range(0, 4):\n",
        "    print('=========== EPOCH {}'.format(epoch))\n",
        "\n",
        "    # Shuffle Train data\n",
        "    np.random.shuffle(train_idx)\n",
        "    print(train_idx[:5])    \n",
        "    print(valid_idx[:5])\n",
        "\n",
        "    # Create Data Generators for Train and Valid\n",
        "    data_generator_train = TrainDataGenerator(train_df.iloc[train_idx], \n",
        "                                                train_df.iloc[train_idx], \n",
        "                                                TRAIN_BATCH_SIZE, \n",
        "                                                SHAPE,\n",
        "                                                augment = True)\n",
        "    data_generator_val = TrainDataGenerator(train_df.iloc[valid_idx], \n",
        "                                            train_df.iloc[valid_idx], \n",
        "                                            VALID_BATCH_SIZE, \n",
        "                                            SHAPE,\n",
        "                                            augment = False)\n",
        "\n",
        "    # Create Model\n",
        "    model = create_model()\n",
        "    \n",
        "    # Full Training Model\n",
        "    for base_layer in model.layers[:-1]:\n",
        "      base_layer.trainable = True\n",
        "      TRAIN_STEPS = int(len(data_generator_train) / 6)\n",
        "      model.load_weights(filepath + \"model.h5\")\n",
        "      # Load Model Weights\n",
        "      '''if epoch != 0:\n",
        "        a = filepath + \"model.h5\"\n",
        "        model.load_weights(a)'''  \n",
        "        \n",
        "\n",
        "    model.compile(optimizer = Adam(LR), \n",
        "                  loss = 'binary_crossentropy',\n",
        "                  metrics = ['acc'])\n",
        "    \n",
        "    # Train Model\n",
        "    model.fit_generator(generator = data_generator_train,\n",
        "                        validation_data = data_generator_val,\n",
        "                        steps_per_epoch = TRAIN_STEPS,\n",
        "                        epochs = 1,\n",
        "                        callbacks = [ModelCheckpointFull('model.h5')],\n",
        "                        verbose = 1)\n",
        "    \n",
        "    # Starting with the 6th epoch we create predictions for the test set on each epoch\n",
        "    if epoch >= 1:\n",
        "        preds = predictions(test_df, model)\n",
        "        submission_predictions.append(preds)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========== EPOCH 0\n",
            "[325589 572890 676207  32943  61492]\n",
            "[ 0  3 17 22 31]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b2_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
            "31940608/31936256 [==============================] - 3s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/1\n",
            "3346/3346 [==============================] - 5681s 2s/step - loss: 0.0775 - acc: 0.9723 - val_loss: 0.0709 - val_acc: 0.9745\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/model.h5\n",
            "=========== EPOCH 1\n",
            "[197400 328264 188772 460826 225076]\n",
            "[ 0  3 17 22 31]\n",
            "Epoch 1/1\n",
            "3346/3346 [==============================] - 5673s 2s/step - loss: 0.0757 - acc: 0.9730 - val_loss: 0.0719 - val_acc: 0.9742\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/model.h5\n",
            "24247/24247 [==============================] - 2050s 85ms/step\n",
            "=========== EPOCH 2\n",
            "[191670 749445 105028 290364 473925]\n",
            "[ 0  3 17 22 31]\n",
            "Epoch 1/1\n",
            "3346/3346 [==============================] - 5744s 2s/step - loss: 0.0743 - acc: 0.9735 - val_loss: 0.0695 - val_acc: 0.9752\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/model.h5\n",
            "24247/24247 [==============================] - 2084s 86ms/step\n",
            "=========== EPOCH 3\n",
            "[352673 474931 618676 188245 415874]\n",
            "[ 0  3 17 22 31]\n",
            "Epoch 1/1\n",
            "3346/3346 [==============================] - 5661s 2s/step - loss: 0.0723 - acc: 0.9741 - val_loss: 0.0713 - val_acc: 0.9745\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/model.h5\n",
            " 1607/24247 [>.............................] - ETA: 33:52Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjBH-1Ia7YQW",
        "colab_type": "code",
        "outputId": "ef69f825-b6c6-497f-8dca-b5ddca37aa39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "test_df.iloc[:, :] = np.average(submission_predictions, axis = 0, weights = [2**i for i in range(len(submission_predictions))])\n",
        "test_df = test_df.stack().reset_index()\n",
        "test_df.insert(loc = 0, column = 'ID', value = test_df['Image'].astype(str) + \"_\" + test_df['Diagnosis'])\n",
        "test_df = test_df.drop([\"Image\", \"Diagnosis\"], axis=1)\n",
        "test_df.to_csv('submission.csv', index = False)\n",
        "print(test_df.head(12))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                               ID         Label\n",
            "0                ID_000000e27_any  6.854479e-02\n",
            "1           ID_000000e27_epidural  2.265973e-03\n",
            "2   ID_000000e27_intraparenchymal  4.580519e-03\n",
            "3   ID_000000e27_intraventricular  1.866179e-04\n",
            "4       ID_000000e27_subarachnoid  4.770795e-02\n",
            "5           ID_000000e27_subdural  2.856068e-02\n",
            "6                ID_000009146_any  6.276369e-05\n",
            "7           ID_000009146_epidural  1.605068e-06\n",
            "8   ID_000009146_intraparenchymal  6.513936e-07\n",
            "9   ID_000009146_intraventricular  3.789152e-07\n",
            "10      ID_000009146_subarachnoid  5.462340e-06\n",
            "11          ID_000009146_subdural  6.846019e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCnE-sUd7YOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!cp submission.csv drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1ZFIUkF7YMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}